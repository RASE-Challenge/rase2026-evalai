---
# ============================
# RASE 2026 Challenge (EvalAI)
# ============================

title: "RASE 2026: Radar Acoustic Speech Enhancement"
short_description: "Recover clean speech from mmWave radar through glass barriers."
description: templates/description.html
evaluation_details: templates/evaluation_details.html
terms_and_conditions: templates/terms_and_conditions.html
submission_guidelines: templates/submission_guidelines.html
leaderboard_description: "Primary leaderboard uses PESQ, ESTOI, DNSMOS, MFCC Cosine Similarity, and a weighted Final score."
image: logo.jpg

# REQUIRED: zip at repo root that contains evaluation_script/ with __init__.py and main.py
evaluation_script: evaluation_script.zip

# Remote worker evaluation
remote_evaluation: true

# Challenge visibility timeframe (UTC)
start_date: "2025-09-07 00:00:00"
end_date:   "2026-02-10 23:59:59"

published: true
tags:
  - speech-enhancement
  - mmwave
  - radar
  - audio

# ---------------------
# Leaderboard definition
# ---------------------
leaderboard:
  - id: 1
    schema:
      labels: ["PESQ", "ESTOI", "DNSMOS", "MFCC-CS", "Total"]
      default_order_by: "Total"
      metadata:
        PESQ:
          sort_ascending: false
          description: "Perceptual Evaluation of Speech Quality (weighted across difficulties)."
        ESTOI:
          sort_ascending: false
          description: "Extended STOI (weighted)."
        DNSMOS:
          sort_ascending: false
          description: "DNSMOS (weighted)."
        MFCC-CS:
          sort_ascending: false
          description: "MFCC cosine similarity (weighted)."
        Total:
          sort_ascending: false
          description: "Aggregate of the four weighted metrics."

# ---------------------
# Dataset split(s)
# ---------------------
dataset_splits:
  - id: 1
    name: "Test"
    codename: "test"

# ---------------------
# Phases (single-phase)
# ---------------------
challenge_phases:
  - id: 1
    name: "Main Phase"
    description: templates/challenge_phase_1_description.html   # must exist
    leaderboard_public: true
    is_public: true
    challenge: 1
    is_active: true

    # Docker + remote eval: submissions are docker images pushed via evalai CLI
    is_docker_based: false

    # submission concurrency and caps
    max_concurrent_submissions_allowed: 2
    max_submissions_per_day: 3
    max_submissions_per_month: 30
    max_submissions: 50

    allowed_email_ids: []
    disable_logs: false
    is_submission_public: false

    start_date: "2025-09-07 00:00:00"
    end_date:   "2026-02-10 23:59:59"

    # Annotation file must exist in the bundle
    test_annotation_file: annotations/test_annotations_devsplit.json

    # This must match what your evaluation script expects
    codename: "main_phase"

    default_submission_meta_attributes:
      - name: method_name
        is_visible: true
      - name: method_description
        is_visible: true
      - name: project_url
        is_visible: true
      - name: publication_url
        is_visible: true

    submission_meta_attributes: []
    is_restricted_to_select_one_submission: false
    is_partial_submission_evaluation_enabled: false
    allowed_submission_file_types: ".json, .zip, .txt, .tsv, .gz, .csv, .h5, .npy, .npz"

# -------------------------
# Phase ↔ Split ↔ Leaderboard
# -------------------------
challenge_phase_splits:
  - challenge_phase_id: 1
    leaderboard_id: 1
    dataset_split_id: 1
    visibility: 3
    leaderboard_decimal_precision: 3
    is_leaderboard_order_descending: true
    show_execution_time: true
    show_leaderboard_by_latest_submission: true





